/**
 * SEC Company Facts Sector Enrichment Service (OPTIMIZED with BATCHING)
 * Uses SEC tickers + Financial Datasets API for sector data
 * Flow: CUSIP/Name ‚Üí SEC Ticker ‚Üí Financial Datasets API ‚Üí Sector
 *
 * OPTIMIZATIONS:
 * - Batch processing (50 tickers per batch)
 * - Parallel batch execution (5 concurrent batches)
 * - ~15-20x faster than sequential processing
 */

import axios from "axios";
import fs from "fs/promises";
import path from "path";
import { fileURLToPath } from "url";
import pMap from "p-map";
import logger from "../../utils/logger.js";
import { getTickerInfoByCusipOrName } from "./sec-tickers-cache.service.js";
import type { DeduplicatedHolding, SectorData } from "../../types/institutional.types.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const FINANCIAL_DATASETS_API_URL = "https://api.financialdatasets.ai/company/facts";
const SECTOR_CACHE_FILE = path.join(__dirname, "../../data/sec-sector-cache.json");
const BATCH_SIZE = 50; // Process 50 tickers per batch
const BATCH_CONCURRENCY = 2; // Process 2 batches in parallel (reduced from 5 to avoid rate limits)

// In-memory cache for sector data
const sectorCache = new Map<string, SectorData | null>();
let cacheLoaded = false;

/**
 * Main function to enrich holdings with sector data (BATCHED & PARALLEL)
 * Flow: CUSIP/Name ‚Üí SEC Ticker ‚Üí Financial Datasets API ‚Üí Sector
 */
export async function enrichHoldingsWithSECSectors(
  holdings: DeduplicatedHolding[],
): Promise<DeduplicatedHolding[]> {
  const apiKey = process.env.FINANCIAL_DATASETS_API_KEY;

  if (!apiKey) {
    logger.warn("‚ö†Ô∏è  FINANCIAL_DATASETS_API_KEY not found, sectors will be Unknown");
    return holdings.map((h) => ({ ...h, sector: "Unknown" }));
  }

  console.log(`\nüîÑ Enriching ${holdings.length} holdings with sector data...`);

  // Load sector cache from disk
  await loadSectorCache();

  // PHASE 1: Resolve tickers for all holdings
  const holdingsWithTickers: Array<{ holding: DeduplicatedHolding; ticker?: string }> = [];
  let failed = 0;

  for (const holding of holdings) {
    const tickerInfo = await getTickerInfoByCusipOrName(holding.cusip, holding.issuerName);
    if (tickerInfo) {
      holdingsWithTickers.push({ holding, ticker: tickerInfo.ticker });
    } else {
      holdingsWithTickers.push({ holding, ticker: undefined });
      failed++;
    }
  }

  // PHASE 2: Separate cached vs needs-lookup
  const cachedHoldings: DeduplicatedHolding[] = [];
  const needsLookup: Array<{ holding: DeduplicatedHolding; ticker: string }> = [];
  let cacheHits = 0;

  for (const { holding, ticker } of holdingsWithTickers) {
    if (!ticker) {
      cachedHoldings.push({ ...holding, ticker: undefined, sector: "Unknown" });
      continue;
    }

    const cacheKey = ticker.toUpperCase();
    const cachedSector = sectorCache.get(cacheKey);

    if (cachedSector !== undefined) {
      // Cache hit
      cacheHits++;
      cachedHoldings.push({
        ...holding,
        ticker,
        sector: cachedSector?.sector || "Unknown",
      });
    } else {
      // Needs API lookup
      needsLookup.push({ holding, ticker });
    }
  }

  console.log(
    `   üìä Status: ${cacheHits} cached, ${needsLookup.length} need lookup, ${failed} failed ticker resolution`,
  );

  // PHASE 3: Batch lookup holdings into groups
  const batches: Array<Array<{ holding: DeduplicatedHolding; ticker: string }>> = [];
  for (let i = 0; i < needsLookup.length; i += BATCH_SIZE) {
    batches.push(needsLookup.slice(i, i + BATCH_SIZE));
  }

  console.log(
    `   üöÄ Processing ${batches.length} batches (${BATCH_SIZE} tickers each) with ${BATCH_CONCURRENCY} parallel workers...`,
  );

  // PHASE 4: Process batches in parallel
  let processedBatches = 0;
  const enrichedFromAPI: DeduplicatedHolding[] = [];

  await pMap(
    batches,
    async (batch) => {
      const batchResults = await processBatch(batch, apiKey);
      enrichedFromAPI.push(...batchResults);
      processedBatches++;
      process.stdout.write(`\r   Progress: ${processedBatches}/${batches.length} batches   `);
    },
    { concurrency: BATCH_CONCURRENCY },
  );

  console.log(""); // New line after progress

  // Combine cached + API results
  const allEnriched = [...cachedHoldings, ...enrichedFromAPI];

  // Save updated cache to disk
  await saveSectorCache();

  const enrichedCount = allEnriched.filter((h) => h.sector && h.sector !== "Unknown").length;
  logger.info(
    `‚úÖ Sector Enrichment Complete: ${enrichedCount}/${holdings.length} (${((enrichedCount / holdings.length) * 100).toFixed(1)}%)`,
  );
  logger.info(`   Cache hits: ${cacheHits}, New lookups: ${needsLookup.length}, Failed: ${failed}`);

  return allEnriched;
}

/**
 * Process a batch of holdings (fetch sectors for all tickers in batch)
 */
async function processBatch(
  batch: Array<{ holding: DeduplicatedHolding; ticker: string }>,
  apiKey: string,
): Promise<DeduplicatedHolding[]> {
  const results: DeduplicatedHolding[] = [];

  // Process all tickers in this batch in parallel
  await pMap(
    batch,
    async ({ holding, ticker }) => {
      try {
        const sectorInfo = await fetchSectorFromFinancialDatasets(ticker, apiKey);

        // Cache the result
        sectorCache.set(ticker.toUpperCase(), sectorInfo);

        results.push({
          ...holding,
          ticker,
          sector: sectorInfo?.sector || "Unknown",
        });
      } catch (error: any) {
        logger.debug(`Error fetching sector for ${ticker}: ${error.message}`);
        sectorCache.set(ticker.toUpperCase(), null);
        results.push({
          ...holding,
          ticker,
          sector: "Unknown",
        });
      }
    },
    { concurrency: 3 }, // 3 API calls in parallel per batch (reduced from 10 to avoid rate limits)
  );

  return results;
}

/**
 * Fetch sector info from Financial Datasets API (NO DELAYS - handled by batching)
 */
async function fetchSectorFromFinancialDatasets(
  ticker: string,
  apiKey: string,
): Promise<SectorData | null> {
  try {
    const response = await axios.get(FINANCIAL_DATASETS_API_URL, {
      params: { ticker },
      headers: { "X-API-KEY": apiKey },
      timeout: 10000,
    });

    const data = response.data;

    if (data?.company_facts?.sector) {
      return {
        ticker: ticker.toUpperCase(),
        sector: data.company_facts.sector,
        industry: data.company_facts.industry,
        marketCap: data.company_facts.market_cap,
      };
    }

    return null;
  } catch (error: any) {
    if (error.response?.status === 429) {
      logger.warn(`‚ö†Ô∏è  Rate limited on ${ticker}, will retry with backoff...`);
      // Wait 2 seconds and retry once (increased from 500ms to be more respectful)
      await sleep(2000);
      try {
        const retryResponse = await axios.get(FINANCIAL_DATASETS_API_URL, {
          params: { ticker },
          headers: { "X-API-KEY": apiKey },
          timeout: 10000,
        });
        if (retryResponse.data?.company_facts?.sector) {
          return {
            ticker: ticker.toUpperCase(),
            sector: retryResponse.data.company_facts.sector,
            industry: retryResponse.data.company_facts.industry,
            marketCap: retryResponse.data.company_facts.market_cap,
          };
        }
      } catch (retryError) {
        logger.debug(`‚ùå ${ticker}: Retry failed after rate limit`);
      }
    }
    return null;
  }
}

/**
 * Load sector cache from disk
 */
async function loadSectorCache(): Promise<void> {
  if (cacheLoaded) return;

  try {
    const data = await fs.readFile(SECTOR_CACHE_FILE, "utf-8");
    const parsed = JSON.parse(data);

    // Rebuild Map from saved object
    for (const [key, value] of Object.entries(parsed)) {
      sectorCache.set(key, value as SectorData | null);
    }

    logger.info(`üìÇ Loaded ${sectorCache.size} cached sectors from disk`);
  } catch (error) {
    logger.info("üìÇ No sector cache found, starting fresh");
  }

  cacheLoaded = true;
}

/**
 * Save sector cache to disk
 */
async function saveSectorCache(): Promise<void> {
  try {
    const dataDir = path.dirname(SECTOR_CACHE_FILE);
    await fs.mkdir(dataDir, { recursive: true });

    // Convert Map to object for JSON serialization
    const cacheObject = Object.fromEntries(sectorCache.entries());

    await fs.writeFile(SECTOR_CACHE_FILE, JSON.stringify(cacheObject, null, 2), "utf-8");
    logger.info(`üíæ Saved ${sectorCache.size} sectors to cache`);
  } catch (error: any) {
    logger.warn(`Failed to save sector cache: ${error.message}`);
  }
}

/**
 * Sleep utility
 */
function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}
